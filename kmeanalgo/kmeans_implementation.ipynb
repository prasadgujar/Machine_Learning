{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2727272727272727\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, cross_validation\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class K_Means:\n",
    "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
    "        self.k = k\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self,data):\n",
    "\n",
    "        self.centroids = {}\n",
    "\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i]\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            self.classifications = {}\n",
    "\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "\n",
    "            for featureset in X:\n",
    "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
    "                classification = distances.index(min(distances))\n",
    "                self.classifications[classification].append(featureset)\n",
    "\n",
    "            prev_centroids = dict(self.centroids)\n",
    "\n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
    "\n",
    "            optimized = True\n",
    "\n",
    "            for c in self.centroids:\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
    "                    #print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
    "                    optimized = False\n",
    "\n",
    "            if optimized:\n",
    "                break\n",
    "\n",
    "    def predict(self,data):\n",
    "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        return classification\n",
    "\n",
    "\n",
    "df = pd.read_excel('titanic.xls')\n",
    "df.drop(['body','name'], 1, inplace=True)\n",
    "#df.convert_objects(convert_numeric=True)\n",
    "#print(df.head())\n",
    "df.fillna(0,inplace=True)\n",
    "\n",
    "def handle_non_numerical_data(df):\n",
    "    \n",
    "    # handling non-numerical data: must convert.\n",
    "    columns = df.columns.values\n",
    "\n",
    "    for column in columns:\n",
    "        text_digit_vals = {}\n",
    "        def convert_to_int(val):\n",
    "            return text_digit_vals[val]\n",
    "\n",
    "        #print(column,df[column].dtype)\n",
    "        if df[column].dtype != np.int64 and df[column].dtype != np.float64:\n",
    "            \n",
    "            column_contents = df[column].values.tolist()\n",
    "            #finding just the uniques\n",
    "            unique_elements = set(column_contents)\n",
    "            # great, found them. \n",
    "            x = 0\n",
    "            for unique in unique_elements:\n",
    "                if unique not in text_digit_vals:\n",
    "                    # creating dict that contains new\n",
    "                    # id per unique string\n",
    "                    text_digit_vals[unique] = x\n",
    "                    x+=1\n",
    "            # now we map the new \"id\" vlaue\n",
    "            # to replace the string. \n",
    "            df[column] = list(map(convert_to_int,df[column]))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = handle_non_numerical_data(df)\n",
    "#print(df.head())\n",
    "\n",
    "# add/remove features just to see impact they have.\n",
    "df.drop(['ticket','home.dest'], 1, inplace=True)\n",
    "\n",
    "\n",
    "X = np.array(df.drop(['survived'], 1).astype(float))\n",
    "X = preprocessing.scale(X)\n",
    "y = np.array(df['survived'])\n",
    "\n",
    "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "clf = K_Means()\n",
    "clf.fit(X)\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(X)):\n",
    "\n",
    "    predict_me = np.array(X[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = clf.predict(predict_me)\n",
    "    if prediction == y[i]:\n",
    "        correct += 1\n",
    "\n",
    "\n",
    "print(correct/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
